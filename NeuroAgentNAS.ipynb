{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Tanh\n",
    "from stable_baselines3 import PPO\n",
    "from dm_control import suite\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import hashlib\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMControlWrapper(gym.Env):\n",
    "    def __init__(self, domain_name, task_name):\n",
    "        self.env = suite.load(domain_name=domain_name, task_name=task_name)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=self.env.action_spec().shape, dtype=np.float32)\n",
    "        \n",
    "        # Assume all observations are concatenated into a single vector\n",
    "        obs_spec = self.env.observation_spec()\n",
    "        obs_dim = sum(np.prod(spec.shape) for spec in obs_spec.values())\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        time_step = self.env.step(action)\n",
    "        reward = time_step.reward if time_step.reward is not None else 0\n",
    "        terminated = time_step.last()\n",
    "        truncated = False  # Set to True if you have a specific truncation condition\n",
    "        \n",
    "        # Concatenate all observations into a single vector\n",
    "        obs = self._flatten_observation(time_step.observation)\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            self.env.task.random.seed(seed)\n",
    "        time_step = self.env.reset()\n",
    "        obs = self._flatten_observation(time_step.observation)\n",
    "        return obs, {}  # Return observation and empty info dict\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.physics.render()\n",
    "\n",
    "    def _flatten_observation(self, observation):\n",
    "        # Flatten the observation dictionary into a single numpy array\n",
    "        return np.concatenate([np.array(value).flatten() for value in observation.values()])\n",
    "\n",
    "# Usage\n",
    "env = DMControlWrapper('cheetah', 'run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Controller, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.fc_layers = nn.Linear(hidden_size, 5)  # Predefined options: 1, 2, or 3 layers\n",
    "        self.fc_units = nn.Linear(hidden_size, 3)  # Predefined options: 64, 128, or 256 units\n",
    "        self.fc_activation = nn.Linear(hidden_size, 3)  # Predefined options: ReLU or Tanh\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out_layers = self.fc_layers(out)\n",
    "        out_units = self.fc_units(out)\n",
    "        out_activation = self.fc_activation(out)\n",
    "        return out_layers, out_units, out_activation, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size))\n",
    "\n",
    "    def sample_architecture(self, temperature=1.2, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Generate a valid architecture for Stable-Baselines3.\n",
    "        Returns a list of dictionaries specifying shared and policy/value networks.\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            num_layers = random.choice([1, 2, 3, 4, 5])\n",
    "            units = random.choice([64, 128, 256])\n",
    "            activation_func = random.choice(['relu', 'leaky_relu', 'swish'])\n",
    "\n",
    "            pi = [units] * num_layers\n",
    "            vf = [units] * num_layers\n",
    "\n",
    "            architecture = dict(pi=pi, vf=vf, activation=activation_func)\n",
    "\n",
    "            return architecture, torch.tensor(0.0), torch.tensor(0.0)\n",
    "\n",
    "        hidden = self.init_hidden()\n",
    "        x = torch.zeros(1, 1, 10)  # Dummy input\n",
    "        \n",
    "        out_layers, out_units, out_activation, _ = self.forward(x, hidden)\n",
    "        \n",
    "        layers_logits = out_layers.squeeze() / temperature\n",
    "        units_logits = out_units.squeeze() / temperature\n",
    "        activation_logits = out_activation.squeeze() / temperature\n",
    "        \n",
    "        # Sample from the outputs\n",
    "        layers_probs = torch.softmax(out_layers.squeeze(), dim=0)\n",
    "        units_probs = torch.softmax(out_units.squeeze(), dim=0)\n",
    "        activation_probs = torch.softmax(out_activation.squeeze(), dim=0)\n",
    "        \n",
    "        entropy_layers = - (layers_probs * torch.log(layers_probs + 1e-8)).sum()\n",
    "        entropy_units = - (units_probs * torch.log(units_probs + 1e-8)).sum()\n",
    "        entropy_activation = - (activation_probs * torch.log(activation_probs + 1e-8)).sum()\n",
    "        total_entropy = entropy_layers + entropy_units + entropy_activation\n",
    "\n",
    "        # layers_idx = torch.multinomial(layers_probs, 1).item()\n",
    "        units_idx = torch.multinomial(units_probs, 1).item()\n",
    "        activation_idx = torch.multinomial(activation_probs, 1).item()\n",
    "        \n",
    "        # Convert indices to actual values\n",
    "        layers_idx = torch.multinomial(layers_probs, 1).item()\n",
    "        num_layers = random.randint(1, 5)  # 1, 2, or 3 layers\n",
    "        # units = [64, 128, 256][units_idx]  # 64, 128, or 256 units\n",
    "        activation_func = ['relu', 'leaky_relu', 'swish'][activation_idx]  # Select activation function\n",
    "\n",
    "        # Save log_probs for later loss calculation\n",
    "        log_prob_layers = torch.log(layers_probs[layers_idx])\n",
    "        log_prob_units = torch.log(units_probs[units_idx])\n",
    "        log_prob_activation = torch.log(activation_probs[activation_idx])\n",
    "\n",
    "        log_prob_sum = log_prob_layers + log_prob_units + log_prob_activation\n",
    "\n",
    "        num_layers = layers_idx + 1\n",
    "        units_options = [64, 128, 256]\n",
    "        units = units_options[units_idx]\n",
    "        activations = ['relu', 'leaky_relu', 'swish']\n",
    "        activation_func = activations[activation_idx]\n",
    "\n",
    "        pi = [units] * num_layers\n",
    "        vf = [units] * num_layers\n",
    "            \n",
    "        # Format: [dict(pi=[...], vf=[...])]\n",
    "        architecture = dict(\n",
    "            pi=pi,  # Policy network\n",
    "            vf=vf,  # Value network\n",
    "            activation=activation_func\n",
    "        )\n",
    "        \n",
    "        return architecture, log_prob_sum, total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_function(name):\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU  # ✅ no parentheses!\n",
    "    elif name == 'leaky_relu':\n",
    "        return nn.LeakyReLU\n",
    "    elif name == 'swish':\n",
    "        return nn.SiLU  # Swish = SiLU\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation function: {name}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def train_agent(env, architecture, total_timesteps=5000):\n",
    "    \"\"\"\n",
    "    Trains a PPO agent with the given architecture on the provided environment.\n",
    "    \n",
    "    :param env: The gym environment.\n",
    "    :param architecture: A dictionary defining the neural network architecture.\n",
    "    :param total_timesteps: Total timesteps to train the agent.\n",
    "    :return: Trained PPO agent.\n",
    "    \"\"\"\n",
    "    # Wrap the environment in a DummyVecEnv\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Define the policy_kwargs with the architecture\n",
    "    pi_layers = architecture['pi']\n",
    "    vf_layers = architecture['vf']\n",
    "    activation = architecture['activation']\n",
    "\n",
    "    # Build proper policy_kwargs\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=[dict(pi=pi_layers, vf=vf_layers)],\n",
    "        activation_fn=get_activation_function(activation)\n",
    "    )\n",
    "    \n",
    "    print(\"Training with architecture:\", architecture)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", vec_env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "    model.learn(total_timesteps=total_timesteps)    \n",
    "    return model\n",
    "\n",
    "def evaluate_agent(model, env, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate a trained RL agent.\n",
    "    \n",
    "    :param model: Trained PPO model\n",
    "    :param env: Gym environment (must be unwrapped or DummyVecEnv)\n",
    "    :param n_episodes: Number of episodes to run\n",
    "    :return: Average reward per episode\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward[0] if isinstance(reward, (list, np.ndarray)) else reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    average_reward = sum(total_rewards) / n_episodes\n",
    "    print(f\"Average reward over {n_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "    \n",
    "\n",
    "def run_nas(controller, env, episodes):\n",
    "    optimizer = optim.Adam(controller.parameters(), lr=0.001) # used for Controller update based on reward\n",
    "\n",
    "    last_arch_str = None\n",
    "    same_count = 0\n",
    "    best_reward = -float('inf')\n",
    "    top_architectures = []\n",
    "    rewards_list = []\n",
    "    losses_list = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        architecture, log_prob, entropy = controller.sample_architecture()\n",
    "        arch_str = json.dumps(architecture, sort_keys=True)\n",
    "\n",
    "        print(f\"Testing architecture: {architecture}\")\n",
    "        \n",
    "        if arch_str == last_arch_str:\n",
    "            same_count += 1\n",
    "        else:\n",
    "            same_count = 1\n",
    "            last_arch_str = arch_str\n",
    "\n",
    "        agent = train_agent(env, architecture)\n",
    "        \n",
    "        # Evaluate the agent\n",
    "        vec_env = DummyVecEnv([lambda: env])\n",
    "        obs = vec_env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, rewards, dones, infos = vec_env.step(action)\n",
    "            total_reward += rewards[0]\n",
    "            done = dones[0]\n",
    "        print(f\"Episode {episode} Total Reward: {total_reward}\")\n",
    "        \n",
    "        if same_count >= 5:\n",
    "            print(f\"⚠️ Architecture repeated {same_count} times. Penalizing reward.\")\n",
    "            total_reward *= 0.5 \n",
    "\n",
    "        entropy_coef = 0.05\n",
    "        loss = -log_prob * total_reward - entropy_coef * entropy\n",
    "        \n",
    "        if log_prob.requires_grad:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(\"🟡 Skipped optimizer step (random architecture)\")\n",
    "\n",
    "        print(f\"Episode {episode} Loss: {loss.item()} Reward: {total_reward}\")\n",
    "        rewards_list.append(total_reward)\n",
    "        losses_list.append(loss.item())\n",
    "        top_architectures.append((architecture, total_reward))\n",
    "\n",
    "        # Optionally, save the best controller state during each episode if needed\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            torch.save(controller.state_dict(), 'controller_best.pth')\n",
    "            print(f\"New best architecture found: {architecture} with reward: {total_reward}\")\n",
    "\n",
    "        # Отрисовка графиков\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_list, label='Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward during NAS')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses_list, label='Controller Loss', color='orange')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Controller Loss during NAS')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sort the architectures based on reward (descending order)\n",
    "    top_architectures.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 7 architectures\n",
    "    top_architectures = top_architectures[:5]\n",
    "    print(f\"Top architectures: {top_architectures}\")\n",
    "    return top_architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "env = DMControlWrapper('cheetah', 'run')\n",
    "controller = Controller(input_size=10, hidden_size=32, num_layers=1)\n",
    "if os.path.exists('controller_best.pth'):\n",
    "    controller.load_state_dict(torch.load('controller_best.pth'))\n",
    "    print(f\"Controller state loaded from 'controller_best.pth'\")\n",
    "else:\n",
    "    print(f\"No saved controller state found at 'controller_best.pth'\")\n",
    "top_architectures = run_nas(controller, env, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best architecture\n",
    "best_reward = -float('inf')\n",
    "best_architecture = None\n",
    "\n",
    "# Iterate over the top architectures and evaluate each one\n",
    "for i, (architecture, reward) in enumerate(top_architectures):\n",
    "    print(f\"Evaluating architecture {i + 1}: {architecture}\")\n",
    "    \n",
    "    rewards = []\n",
    "    for _ in range(3):\n",
    "        model = train_agent(env, architecture, 30000)\n",
    "        r = evaluate_agent(model, DummyVecEnv([lambda: env]), n_episodes=3)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    print(f\"Architecture {i + 1} average reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "    \n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        best_architecture = architecture\n",
    "        print(f\"New best architecture found: {best_architecture} with avg reward: {best_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "best_model = train_agent(env, best_architecture, total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_architecture = {\n",
    "    'pi': [64, 64],\n",
    "    'vf': [64, 64],\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "test_model_base = train_agent(env, baseline_architecture, total_timesteps = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "evaluate_agent(best_model, DummyVecEnv([lambda: env]), n_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "evaluate_agent(test_model_base, DummyVecEnv([lambda: env]), n_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
