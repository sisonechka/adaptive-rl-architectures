# adaptive-rl-architectures
Adaptive NAS for RL agents using PPO in the DM Control Suite (Cheetah). The system auto-selects neural architectures based on performance, reducing manual tuning and compute cost. Applicable to robotics, autonomous systems, and AI research.

ğŸ“Œ Project Goals
- Automate neural architecture selection for RL agents
- Reduce manual tuning and computation cost
- Improve performance in dynamic and complex environments

ğŸ§© Key Concepts
- Neural Architecture Search (NAS): A recurrent LSTM controller generates candidate architectures by varying layer count, size, and activation functions.
- Reinforcement Learning (RL): PPO is used to train agents on proposed architectures.
- Closed training loop: Architectures are generated â†’ agents are trained â†’ rewards are evaluated â†’ controller is updated.

âš™ï¸ Architecture Parameters
- Layers: 1 to 5
- Layer Sizes: 64, 128, 256
- Activations: ReLU, Leaky ReLU, Swish

Example generated architecture:
{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128], 'activation': 'relu'}


ğŸš€ Training Flow

NAS controller generates candidate architectures

PPO agent is trained on each candidate for a short cycle

Top-performing architectures are selected based on average reward

Final architectures undergo longer training for evaluation

Controller is updated and loop repeats

ğŸ“Š Metrics & Evaluation
- Main metric: Average reward over training steps
- Data and results are serialized to JSON for reproducibility
- Architecture filtering avoids redundant or weak candidates
- Best architecture achieved >5.5 reward at 5000 steps (55%+ gain vs baseline)

ğŸ“ˆ Results
- Significant improvement over baseline architectures
- Efficient filtering reduced training overhead
- Confirmed generalization across different random seeds
- Potential for adaptation to more environments and real-world robotics tasks

ğŸ”® Future Work
- Expand to more agents and environments
- Integrate advanced NAS algorithms and meta-learning
- Apply system to robotics and autonomous navigation

ğŸ› ï¸ Technologies Used
- Python, PyTorch
- DeepMind Control Suite (dm_control)
- Proximal Policy Optimization (PPO)
- LSTM-based NAS controller

ğŸ“‚ Project Structure

â”œâ”€â”€ nas_controller/         # Architecture generator (LSTM)

â”œâ”€â”€ ppo_agent/              # PPO training logic

â”œâ”€â”€ environments/           # DM Control Suite integration

â”œâ”€â”€ data/                   # JSON logs and architecture metrics

â””â”€â”€ utils/                  # Helpers and visualization tools

ğŸ‘¤ Author
S. O. Balaba
Supervisor: Assoc. Prof. M. V. Holovyanko




---

# ğŸ§  Adaptive Neural Architecture Search for RL Agents

This project implements an adaptive system for **automated neural network architecture search (NAS)** for agents in simulated environments using **reinforcement learning (RL)**. The system combines a NAS controller and a PPO-based RL agent trained in the **DeepMind Control Suite** (`cheetah` environment).

---

## ğŸ“Œ Project Goals

* Automate neural architecture selection for RL agents
* Reduce manual tuning and computation cost
* Improve performance in dynamic and complex environments

---

## ğŸ§© Key Concepts

* **Neural Architecture Search (NAS):** A recurrent LSTM controller generates candidate architectures by varying layer count, size, and activation functions.
* **Reinforcement Learning (RL):** PPO is used to train agents on proposed architectures.
* **Closed training loop:** Architectures are generated â†’ agents are trained â†’ rewards are evaluated â†’ controller is updated.

---

## âš™ï¸ Architecture Parameters

* **Layers:** 1 to 5
* **Layer Sizes:** 64, 128, 256
* **Activations:** ReLU, Leaky ReLU, Swish

Example generated architecture:

```json
{
  "layers": [128, 256, 256],
  "activation": "Swish"
}
```

---

## ğŸš€ Training Flow

1. NAS controller generates candidate architectures
2. PPO agent is trained on each candidate for a short cycle
3. Top-performing architectures are selected based on average reward
4. Final architectures undergo longer training for evaluation
5. Controller is updated and loop repeats

---

## ğŸ“Š Metrics & Evaluation

* **Main metric:** Average reward over training steps
* Data and results are serialized to JSON for reproducibility
* Architecture filtering avoids redundant or weak candidates
* Best architecture achieved >5.5 reward at 5000 steps (55%+ gain vs baseline)

---

## ğŸ“ˆ Results

* Significant improvement over baseline architectures
* Efficient filtering reduced training overhead
* Confirmed generalization across different random seeds
* Potential for adaptation to more environments and real-world robotics tasks

---

## ğŸ”® Future Work

* Expand to more agents and environments
* Integrate advanced NAS algorithms and meta-learning
* Apply system to robotics and autonomous navigation

---

## ğŸ› ï¸ Technologies Used

* Python, PyTorch
* DeepMind Control Suite (`dm_control`)
* Proximal Policy Optimization (PPO)
* LSTM-based NAS controller

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ nas_controller/         # Architecture generator (LSTM)
â”œâ”€â”€ ppo_agent/              # PPO training logic
â”œâ”€â”€ environments/           # DM Control Suite integration
â”œâ”€â”€ data/                   # JSON logs and architecture metrics
â””â”€â”€ utils/                  # Helpers and visualization tools
```

---

## ğŸ‘¤ Author

**S. O. Balaba**
Supervisor: Assoc. Prof. M. V. Holovyanko

---
